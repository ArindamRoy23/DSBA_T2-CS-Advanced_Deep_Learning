{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArindamRoy23/DSBA_T2-CS-Advanced_Deep_Learning/blob/master/TP4/TP4_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical session on Transfer Learning**\n",
        "This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."
      ],
      "metadata": {
        "id": "4jVkOWmgFT1p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKnIngy_2hg"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Context :**\n",
        "\n",
        "Assume we are in a context where few \"gold\" labeled data are available for training, say \n",
        "\n",
        "$$\\mathcal{X}_{\\text{train}} = \\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$$\n",
        "\n",
        "where $N_{\\text{train}}$ is small. \n",
        "\n",
        "A large test set $\\mathcal{X}_{\\text{test}}$ as well as a large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
        "\n",
        "**Instructions to follow :** \n",
        "\n",
        "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question :\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   XXX  | XXX | XXX | XXX |\n",
        "\n",
        "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset!)\n",
        "\n",
        "In your final report, please *keep the logs of each training procedure* you used. We will only run this jupyter if we have some doubts on your implementation. \n",
        "\n",
        "The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.\n",
        "\n",
        "You can use https://colab.research.google.com/ to run your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTCQPSh_2hg"
      },
      "source": [
        "## Training set creation\n",
        "__Question 1 (2 points) :__ Propose a dataloader to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set.\n",
        "\n",
        "Additional information :  \n",
        "\n",
        "*   CIFAR10 dataset : https://en.wikipedia.org/wiki/CIFAR-10\n",
        "*   You can directly use the dataloader framework from Pytorch.\n",
        "*   Alternatively you can modify the file : https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZkC5IxR_2hh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset.data = trainset.data[:100]\n",
        "trainset.targets = trainset.targets[:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "valset.data = valset.data[100:150]\n",
        "valset.targets = valset.targets[100:150]\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=10,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "664bef82b06a4ebda0bd919b0e0efe93",
            "b6cd510c2ad64c8b86aa3903c928a93b",
            "09b69c8ad0fb42c491cc08ac427e8be8",
            "59f73bf5ad3446b1962830676993355e",
            "880ff6e6bb714bb7a33dbce4dbc3a76c",
            "4172f32c24b34f1f831ae36e16d54a1b",
            "a4ada85d2ea54b44a8d7f8e7366e1473",
            "e1320e9c258d438aab27dc593fe9bb85",
            "413dec040714434b9e2065b5d377d256",
            "7e4658b4c556474f89cf54f122a83aff",
            "dd9be09960f64fe4b9ee3baec74379cb"
          ]
        },
        "id": "K0m4QxEp4tqW",
        "outputId": "2acdd99c-0120-47f8-91ef-9d8c516c2b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "664bef82b06a4ebda0bd919b0e0efe93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUno1nmu_2hh"
      },
      "source": [
        "* This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. \n",
        "\n",
        "* The remaining samples correspond to $\\mathcal{X}$. \n",
        "\n",
        "* The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr0d4o5L_2hi"
      },
      "source": [
        "## Testing procedure\n",
        "__Question 2 (1.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiTrnpd_2hi"
      },
      "source": [
        "Training a machine learning model is a challenging task, and evaluating the training procedure's effectiveness can be equally difficult, especially in the context of a small dataset. The primary reason is that the model may overfit to the training data, resulting in a high training accuracy but poor generalization performance on new data. In the case of a small dataset, the risk of overfitting is particularly high, and the evaluation of the training procedure's effectiveness can be difficult due to the limited amount of data available for training and validation.\n",
        "\n",
        "* **Use data augmentation techniques** to artificially increase the size of the dataset. Data augmentation involves applying transformations such as rotation, cropping, and flipping to the training images, resulting in a larger and more diverse training set. This approach can improve the model's generalization performance and reduce the risk of overfitting.\n",
        "\n",
        "* **Use transfer learning**, where a pre-trained model is used as a starting point, and the model is fine-tuned on the small dataset. This approach can leverage the knowledge learned from a large dataset to improve the model's performance on the small dataset.\n",
        "\n",
        "* **Regularization techniques** such as dropout or L1/L2 regularization can be used to prevent overfitting, which is particularly important in the context of a small dataset where the risk of overfitting is higher.\n",
        "\n",
        "* **Early stopping** can be employed to prevent the model from overfitting to the training data. By monitoring the model's performance on the validation set during training, the training process can be stopped before the model starts to overfit, resulting in better generalization performance.\n",
        "\n",
        "* **Use semi-supervised learning.** Semi-supervised learning combines both labeled and unlabeled data to improve the accuracy of the model. With semi-supervised learning, the model has access to a larger amount of data and the model can learn from the larger amount of unlabeled data, which can help it better understand the underlying structure of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEaIwILB_2hi"
      },
      "source": [
        "# The Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-PQZ2Vl_2hi"
      },
      "source": [
        "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
        "\n",
        "The key ingredients for training a CNN are the batch size, as well as the learning rate scheduler (i.e. how to decrease the learning rate as a function of the number of epochs). A possible scheduler is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the learning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
        "\n",
        "You can get some baselines accuracies in this paper (obviously, it is a different context for those researchers who had access to GPUs!) : http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rSNtK2ZhgjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "ecCW-sz63IiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "pVa_ZZEzcbAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainloader, valloader, optimizer, scheduler, criterion, patience, device):\n",
        "    # Early stopping parameters\n",
        "    best_accuracy = 0.0\n",
        "    counter = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(100):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move your data and model to GPU if available\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            model.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print statistics\n",
        "        print('[Epoch %d] loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
        "\n",
        "        # Validation testing\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print('[Epoch %d] validation accuracy: %.2f %%' % (epoch + 1, accuracy))\n",
        "\n",
        "        # Early stopping\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            counter = 0\n",
        "            # Save the model weights to a file\n",
        "            torch.save(model.state_dict(), 'best_weights.pth')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print('Early stopping: validation accuracy did not improve for %d epochs.' % patience)\n",
        "                break\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # Load the best model weights and evaluate on the test set\n",
        "    model.load_state_dict(torch.load('best_weights.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "e_dcQyY_dGZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseLine(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseLine, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "S7vArEOx5AcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BaseLine()\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_accuracy = 0.0\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "su46w7hedMd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = train_model(model, trainloader, valloader, optimizer, scheduler, criterion, patience, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYFwiFlCdojf",
        "outputId": "0989a430-3bd5-4c4f-ee5d-81b8018baea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 2.294\n",
            "[Epoch 1] validation accuracy: 16.00 %\n",
            "[Epoch 2] loss: 2.254\n",
            "[Epoch 2] validation accuracy: 16.00 %\n",
            "[Epoch 3] loss: 2.228\n",
            "[Epoch 3] validation accuracy: 16.00 %\n",
            "[Epoch 4] loss: 2.201\n",
            "[Epoch 4] validation accuracy: 16.00 %\n",
            "[Epoch 5] loss: 2.160\n",
            "[Epoch 5] validation accuracy: 16.00 %\n",
            "[Epoch 6] loss: 2.086\n",
            "[Epoch 6] validation accuracy: 24.00 %\n",
            "[Epoch 7] loss: 1.984\n",
            "[Epoch 7] validation accuracy: 28.00 %\n",
            "[Epoch 8] loss: 1.859\n",
            "[Epoch 8] validation accuracy: 26.00 %\n",
            "[Epoch 9] loss: 1.727\n",
            "[Epoch 9] validation accuracy: 26.00 %\n",
            "[Epoch 10] loss: 1.626\n",
            "[Epoch 10] validation accuracy: 22.00 %\n",
            "[Epoch 11] loss: 1.456\n",
            "[Epoch 11] validation accuracy: 26.00 %\n",
            "[Epoch 12] loss: 1.411\n",
            "[Epoch 12] validation accuracy: 30.00 %\n",
            "[Epoch 13] loss: 1.394\n",
            "[Epoch 13] validation accuracy: 28.00 %\n",
            "[Epoch 14] loss: 1.367\n",
            "[Epoch 14] validation accuracy: 30.00 %\n",
            "[Epoch 15] loss: 1.340\n",
            "[Epoch 15] validation accuracy: 28.00 %\n",
            "[Epoch 16] loss: 1.324\n",
            "[Epoch 16] validation accuracy: 30.00 %\n",
            "[Epoch 17] loss: 1.307\n",
            "[Epoch 17] validation accuracy: 30.00 %\n",
            "[Epoch 18] loss: 1.291\n",
            "[Epoch 18] validation accuracy: 30.00 %\n",
            "[Epoch 19] loss: 1.273\n",
            "[Epoch 19] validation accuracy: 28.00 %\n",
            "[Epoch 20] loss: 1.257\n",
            "[Epoch 20] validation accuracy: 28.00 %\n",
            "[Epoch 21] loss: 1.237\n",
            "[Epoch 21] validation accuracy: 28.00 %\n",
            "[Epoch 22] loss: 1.235\n",
            "[Epoch 22] validation accuracy: 28.00 %\n",
            "Early stopping: validation accuracy did not improve for 10 epochs.\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUsvL83h8XRs",
        "outputId": "eaa3ffef-0764-4aa2-8cb8-24faf661c0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 18 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EkCF0SA8lq6",
        "outputId": "12ec98a5-1474-4abf-ba42-d6a38760bb01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of plane :  1 %\n",
            "Accuracy of   car : 30 %\n",
            "Accuracy of  bird : 25 %\n",
            "Accuracy of   cat : 33 %\n",
            "Accuracy of  deer : 18 %\n",
            "Accuracy of   dog :  0 %\n",
            "Accuracy of  frog :  0 %\n",
            "Accuracy of horse : 35 %\n",
            "Accuracy of  ship :  0 %\n",
            "Accuracy of truck : 37 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   BaseLine CNN  | 12 | 20% | 18% |"
      ],
      "metadata": {
        "id": "jwdAX8DcezT2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARHWPXrY_2hi"
      },
      "source": [
        "## ResNet architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMbGoNw_2hj"
      },
      "source": [
        "__Question 3 (4 points) :__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
        "\n",
        "*Hint :* You can re-use the following code : https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "afqdSnAAd-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVHhKmWN_2hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f85f668-b14d-4d8a-d91c-84ecc5f1a65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained ResNet-18 model\n",
        "resnet18 = models.resnet18(pretrained=False)\n",
        "\n",
        "# Replace the last layer to have 10 outputs (one for each class in CIFAR-10)\n",
        "num_features = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "# Print the model architecture\n",
        "print(resnet18)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_accuracy = 0.0\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "CKB22UsWd8ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = train_model(model, trainloader, valloader, optimizer, scheduler, criterion, patience, device)"
      ],
      "metadata": {
        "outputId": "be980b65-7fda-43a1-ae1e-80702f1840a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-I7PaAid8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 2.621\n",
            "[Epoch 1] validation accuracy: 24.00 %\n",
            "[Epoch 2] loss: 1.776\n",
            "[Epoch 2] validation accuracy: 12.00 %\n",
            "[Epoch 3] loss: 1.301\n",
            "[Epoch 3] validation accuracy: 20.00 %\n",
            "[Epoch 4] loss: 0.899\n",
            "[Epoch 4] validation accuracy: 18.00 %\n",
            "[Epoch 5] loss: 0.447\n",
            "[Epoch 5] validation accuracy: 20.00 %\n",
            "[Epoch 6] loss: 0.369\n",
            "[Epoch 6] validation accuracy: 32.00 %\n",
            "[Epoch 7] loss: 0.515\n",
            "[Epoch 7] validation accuracy: 28.00 %\n",
            "[Epoch 8] loss: 0.583\n",
            "[Epoch 8] validation accuracy: 22.00 %\n",
            "[Epoch 9] loss: 0.627\n",
            "[Epoch 9] validation accuracy: 26.00 %\n",
            "[Epoch 10] loss: 0.449\n",
            "[Epoch 10] validation accuracy: 14.00 %\n",
            "[Epoch 11] loss: 0.262\n",
            "[Epoch 11] validation accuracy: 16.00 %\n",
            "[Epoch 12] loss: 0.179\n",
            "[Epoch 12] validation accuracy: 22.00 %\n",
            "[Epoch 13] loss: 0.105\n",
            "[Epoch 13] validation accuracy: 28.00 %\n",
            "[Epoch 14] loss: 0.062\n",
            "[Epoch 14] validation accuracy: 22.00 %\n",
            "[Epoch 15] loss: 0.051\n",
            "[Epoch 15] validation accuracy: 24.00 %\n",
            "[Epoch 16] loss: 0.060\n",
            "[Epoch 16] validation accuracy: 26.00 %\n",
            "Early stopping: validation accuracy did not improve for 10 epochs.\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c393eeff-1c02-449d-8e30-bb61f565f796",
        "id": "YxRIdCk1d8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8044833-605a-4a85-bae9-8f655163e235",
        "id": "rCtNOvSKd8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of plane : 10 %\n",
            "Accuracy of   car : 41 %\n",
            "Accuracy of  bird : 26 %\n",
            "Accuracy of   cat : 14 %\n",
            "Accuracy of  deer : 25 %\n",
            "Accuracy of   dog :  8 %\n",
            "Accuracy of  frog : 30 %\n",
            "Accuracy of horse : 19 %\n",
            "Accuracy of  ship : 29 %\n",
            "Accuracy of truck : 23 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   ResNet18 (Without Pre-Trained)  | 6 | 32% | 22% |"
      ],
      "metadata": {
        "id": "cHLa9Y9qfG6F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C3mHqCk_2hj"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tn9pW14_2hj"
      },
      "source": [
        "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHiAGo_zfdT3"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet-18 model\n",
        "resnet18pre = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace the last layer to have 10 outputs (one for each class in CIFAR-10)\n",
        "num_features = resnet18pre.fc.in_features\n",
        "resnet18pre.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "# Print the model architecture\n",
        "print(resnet18pre)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18pre\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_accuracy = 0.0\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "_xQlprnTfdT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = train_model(model, trainloader, valloader, optimizer, scheduler, criterion, patience, device)"
      ],
      "metadata": {
        "outputId": "5191e569-6c83-4887-8752-df7d6a941466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP49wx4jfdT3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 2.876\n",
            "[Epoch 1] validation accuracy: 22.00 %\n",
            "[Epoch 2] loss: 1.867\n",
            "[Epoch 2] validation accuracy: 28.00 %\n",
            "[Epoch 3] loss: 1.489\n",
            "[Epoch 3] validation accuracy: 24.00 %\n",
            "[Epoch 4] loss: 1.240\n",
            "[Epoch 4] validation accuracy: 34.00 %\n",
            "[Epoch 5] loss: 1.207\n",
            "[Epoch 5] validation accuracy: 30.00 %\n",
            "[Epoch 6] loss: 1.018\n",
            "[Epoch 6] validation accuracy: 38.00 %\n",
            "[Epoch 7] loss: 0.767\n",
            "[Epoch 7] validation accuracy: 38.00 %\n",
            "[Epoch 8] loss: 0.675\n",
            "[Epoch 8] validation accuracy: 30.00 %\n",
            "[Epoch 9] loss: 0.774\n",
            "[Epoch 9] validation accuracy: 36.00 %\n",
            "[Epoch 10] loss: 0.643\n",
            "[Epoch 10] validation accuracy: 40.00 %\n",
            "[Epoch 11] loss: 0.705\n",
            "[Epoch 11] validation accuracy: 48.00 %\n",
            "[Epoch 12] loss: 0.616\n",
            "[Epoch 12] validation accuracy: 48.00 %\n",
            "[Epoch 13] loss: 0.446\n",
            "[Epoch 13] validation accuracy: 36.00 %\n",
            "[Epoch 14] loss: 0.211\n",
            "[Epoch 14] validation accuracy: 40.00 %\n",
            "[Epoch 15] loss: 0.227\n",
            "[Epoch 15] validation accuracy: 38.00 %\n",
            "[Epoch 16] loss: 0.278\n",
            "[Epoch 16] validation accuracy: 40.00 %\n",
            "[Epoch 17] loss: 0.201\n",
            "[Epoch 17] validation accuracy: 32.00 %\n",
            "[Epoch 18] loss: 0.138\n",
            "[Epoch 18] validation accuracy: 50.00 %\n",
            "[Epoch 19] loss: 0.205\n",
            "[Epoch 19] validation accuracy: 28.00 %\n",
            "[Epoch 20] loss: 0.107\n",
            "[Epoch 20] validation accuracy: 44.00 %\n",
            "[Epoch 21] loss: 0.106\n",
            "[Epoch 21] validation accuracy: 46.00 %\n",
            "[Epoch 22] loss: 0.112\n",
            "[Epoch 22] validation accuracy: 50.00 %\n",
            "[Epoch 23] loss: 0.325\n",
            "[Epoch 23] validation accuracy: 36.00 %\n",
            "[Epoch 24] loss: 0.214\n",
            "[Epoch 24] validation accuracy: 38.00 %\n",
            "[Epoch 25] loss: 0.149\n",
            "[Epoch 25] validation accuracy: 36.00 %\n",
            "[Epoch 26] loss: 0.176\n",
            "[Epoch 26] validation accuracy: 36.00 %\n",
            "[Epoch 27] loss: 0.140\n",
            "[Epoch 27] validation accuracy: 44.00 %\n",
            "[Epoch 28] loss: 0.118\n",
            "[Epoch 28] validation accuracy: 38.00 %\n",
            "Early stopping: validation accuracy did not improve for 10 epochs.\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ddb571-7b6e-4861-ac4c-b5505ea3b6af",
        "id": "Pdso5amLfdT3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 29 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae90676-3cf6-470e-c227-1770e048104f",
        "id": "9zXF7WlNfdT3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of plane :  7 %\n",
            "Accuracy of   car : 61 %\n",
            "Accuracy of  bird : 45 %\n",
            "Accuracy of   cat : 23 %\n",
            "Accuracy of  deer : 35 %\n",
            "Accuracy of   dog : 13 %\n",
            "Accuracy of  frog : 28 %\n",
            "Accuracy of horse : 37 %\n",
            "Accuracy of  ship : 14 %\n",
            "Accuracy of truck : 37 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   ResNet18 (With Pre-Trained)  | 10 | 50% | 29% |"
      ],
      "metadata": {
        "id": "QBvrxeYRfdT4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8g_3ZDi_2hj"
      },
      "source": [
        "## ImageNet features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYEhdFb_2hj"
      },
      "source": [
        "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on : https://pytorch.org/vision/stable/models.html.\n",
        "\n",
        "__Question 4 (3 points):__ Pick a model from the list above, adapt it for CIFAR10 and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cc66c381222945e096fa1546581d4e1d",
            "7932e4bb937a4d539081b11a934b0892",
            "32ca89a57cc14e8a9589147e76fa9bc5",
            "dcc7f3bbde8742f69c519a044c3c551c",
            "4be84794e76947a1938fed3fc8cca790",
            "2efd162568be485f9e14c5f81a9323af",
            "17a8f47e81bf4210af52029641173a7e",
            "e510d8729fa34b44853cf10a02a66cec",
            "c3e8ab9e461c4a59a33572df5edc6568",
            "0935ceb23e494f2f9107db6dabf75a1a",
            "217533695ef8487aa5e8a5aed4eac07e"
          ]
        },
        "outputId": "37084f5a-c62c-4208-8aed-722a3ab42da3",
        "id": "xceVTab8f9gv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc66c381222945e096fa1546581d4e1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# Load pre-trained ResNet-50 model\n",
        "resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# Replace the last layer to have 10 outputs (one for each class in CIFAR-10)\n",
        "num_features = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "# Print the model architecture\n",
        "print(resnet50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_accuracy = 0.0\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "t9NuWL_Ff9gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = train_model(model, trainloader, valloader, optimizer, scheduler, criterion, patience, device)"
      ],
      "metadata": {
        "outputId": "299236a3-c460-4d24-83b3-34ae6fddeba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbGPg_OYf9gw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 2.258\n",
            "[Epoch 1] validation accuracy: 16.00 %\n",
            "[Epoch 2] loss: 2.121\n",
            "[Epoch 2] validation accuracy: 22.00 %\n",
            "[Epoch 3] loss: 1.924\n",
            "[Epoch 3] validation accuracy: 16.00 %\n",
            "[Epoch 4] loss: 1.524\n",
            "[Epoch 4] validation accuracy: 26.00 %\n",
            "[Epoch 5] loss: 1.313\n",
            "[Epoch 5] validation accuracy: 32.00 %\n",
            "[Epoch 6] loss: 1.098\n",
            "[Epoch 6] validation accuracy: 26.00 %\n",
            "[Epoch 7] loss: 0.843\n",
            "[Epoch 7] validation accuracy: 16.00 %\n",
            "[Epoch 8] loss: 0.653\n",
            "[Epoch 8] validation accuracy: 26.00 %\n",
            "[Epoch 9] loss: 0.487\n",
            "[Epoch 9] validation accuracy: 26.00 %\n",
            "[Epoch 10] loss: 0.408\n",
            "[Epoch 10] validation accuracy: 34.00 %\n",
            "[Epoch 11] loss: 0.364\n",
            "[Epoch 11] validation accuracy: 26.00 %\n",
            "[Epoch 12] loss: 0.314\n",
            "[Epoch 12] validation accuracy: 28.00 %\n",
            "[Epoch 13] loss: 0.288\n",
            "[Epoch 13] validation accuracy: 30.00 %\n",
            "[Epoch 14] loss: 0.199\n",
            "[Epoch 14] validation accuracy: 38.00 %\n",
            "[Epoch 15] loss: 0.193\n",
            "[Epoch 15] validation accuracy: 22.00 %\n",
            "[Epoch 16] loss: 0.220\n",
            "[Epoch 16] validation accuracy: 26.00 %\n",
            "[Epoch 17] loss: 0.214\n",
            "[Epoch 17] validation accuracy: 38.00 %\n",
            "[Epoch 18] loss: 0.210\n",
            "[Epoch 18] validation accuracy: 26.00 %\n",
            "[Epoch 19] loss: 0.137\n",
            "[Epoch 19] validation accuracy: 34.00 %\n",
            "[Epoch 20] loss: 0.084\n",
            "[Epoch 20] validation accuracy: 26.00 %\n",
            "[Epoch 21] loss: 0.200\n",
            "[Epoch 21] validation accuracy: 34.00 %\n",
            "[Epoch 22] loss: 0.092\n",
            "[Epoch 22] validation accuracy: 30.00 %\n",
            "[Epoch 23] loss: 0.200\n",
            "[Epoch 23] validation accuracy: 22.00 %\n",
            "[Epoch 24] loss: 0.127\n",
            "[Epoch 24] validation accuracy: 30.00 %\n",
            "Early stopping: validation accuracy did not improve for 10 epochs.\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c413a8cb-7c5a-4539-a0d4-d831fecf3d0f",
        "id": "MpshEvmcf9gw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 30 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7810a29-d67a-44d5-c44b-1f7dd1f5afb9",
        "id": "YYQczTLWf9gw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of plane :  8 %\n",
            "Accuracy of   car : 49 %\n",
            "Accuracy of  bird : 38 %\n",
            "Accuracy of   cat : 25 %\n",
            "Accuracy of  deer : 48 %\n",
            "Accuracy of   dog : 16 %\n",
            "Accuracy of  frog : 18 %\n",
            "Accuracy of horse : 33 %\n",
            "Accuracy of  ship : 20 %\n",
            "Accuracy of truck : 47 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   ResNet50 (ImageNet weights)  | 14 | 38% | 30% |"
      ],
      "metadata": {
        "id": "o_E2yJBWf9gw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvkuMzLs_2hk"
      },
      "source": [
        "# Incorporating *a priori*\n",
        "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that :\n",
        "\n",
        "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
        "\n",
        "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to :\n",
        "\n",
        "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
        "\n",
        "Otherwise, one has to handle several boundary effects.\n",
        "\n",
        "__Question 5 (1.5 points) :__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIaY60o1_2hk"
      },
      "source": [
        "Dealing with transformations on $32\\times32$ images can pose several issues in image classification tasks:\n",
        "\n",
        "* **Translations:** When dealing with translations, the image content changes, and therefore the classification model may not recognize it. \n",
        " \n",
        " This can be tackled by data augmentation techniques like random cropping and horizontal flipping of images to make the model more robust to translations.\n",
        "\n",
        "* **Rotations:** Images can also be rotated by an arbitrary angle, which can pose issues since the pixels may not align correctly. \n",
        "\n",
        " This can be handled by using rotation augmentation techniques during training to train the model to be more robust to rotations.\n",
        "\n",
        "* **Scaling effects:** Scaling effects can change the size of an image and it can also pose issues in image classification. \n",
        "\n",
        " Aapplying random scaling during training can make the model robust to scaling effects.\n",
        "\n",
        "* **Color changes:** Color changes in images can change the pixel values of the image, which can also cause issues. \n",
        "\n",
        " This can be tackled by using color augmentation techniques during training.\n",
        "\n",
        "In summary, the ideas to tackle these issues include:\n",
        "\n",
        "* Data augmentation: Data augmentation techniques can be used to artificially increase the size of the training dataset, and this can make the model more robust to translations, rotations, scaling effects, and color changes.\n",
        "\n",
        "* Normalization: Normalization techniques can be used to scale the pixel values of the images to a common range, and this can make the model more robust to color changes.\n",
        "\n",
        "* Use of advanced architectures: Advanced architectures like ResNet, DenseNet, etc., can be used to handle these issues. These architectures use skip connections, and this makes them more robust to transformations.\n",
        "\n",
        "* Use of ensembles: Ensembles of models can be used to handle these issues. Since each model may be better at handling certain transformations, using an ensemble of models can help to mitigate the effects of these transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6e6teG_2hk"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ek5wlOo_2hk"
      },
      "source": [
        "__Question 6 (3 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FqCjrXGk_2hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc9945c-24c6-4ae5-a1bc-63b6656def7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainset.data = trainset.data[:100]\n",
        "trainset.targets = trainset.targets[:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=val_transform)\n",
        "valset.data = valset.data[100:150]\n",
        "valset.targets = valset.targets[100:150]\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=10,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=val_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35437ceb-4342-43e4-cd3b-93b9418d1dcf",
        "id": "foKMtUk1g9h5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# Load pre-trained ResNet-50 model\n",
        "resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# Replace the last layer to have 10 outputs (one for each class in CIFAR-10)\n",
        "num_features = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "# Print the model architecture\n",
        "print(resnet50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_accuracy = 0.0\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "dAiAFvIpg9h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = train_model(model, trainloader, valloader, optimizer, scheduler, criterion, patience, device)"
      ],
      "metadata": {
        "outputId": "da03df2a-457f-428a-e2eb-61b0de5df406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5cZH1LSg9h5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 2.318\n",
            "[Epoch 1] validation accuracy: 6.00 %\n",
            "[Epoch 2] loss: 2.236\n",
            "[Epoch 2] validation accuracy: 18.00 %\n",
            "[Epoch 3] loss: 2.109\n",
            "[Epoch 3] validation accuracy: 22.00 %\n",
            "[Epoch 4] loss: 2.069\n",
            "[Epoch 4] validation accuracy: 20.00 %\n",
            "[Epoch 5] loss: 1.991\n",
            "[Epoch 5] validation accuracy: 24.00 %\n",
            "[Epoch 6] loss: 1.775\n",
            "[Epoch 6] validation accuracy: 30.00 %\n",
            "[Epoch 7] loss: 1.855\n",
            "[Epoch 7] validation accuracy: 28.00 %\n",
            "[Epoch 8] loss: 1.688\n",
            "[Epoch 8] validation accuracy: 36.00 %\n",
            "[Epoch 9] loss: 1.762\n",
            "[Epoch 9] validation accuracy: 42.00 %\n",
            "[Epoch 10] loss: 1.732\n",
            "[Epoch 10] validation accuracy: 20.00 %\n",
            "[Epoch 11] loss: 1.522\n",
            "[Epoch 11] validation accuracy: 28.00 %\n",
            "[Epoch 12] loss: 1.438\n",
            "[Epoch 12] validation accuracy: 32.00 %\n",
            "[Epoch 13] loss: 1.410\n",
            "[Epoch 13] validation accuracy: 36.00 %\n",
            "[Epoch 14] loss: 1.388\n",
            "[Epoch 14] validation accuracy: 34.00 %\n",
            "[Epoch 15] loss: 1.414\n",
            "[Epoch 15] validation accuracy: 30.00 %\n",
            "[Epoch 16] loss: 1.273\n",
            "[Epoch 16] validation accuracy: 32.00 %\n",
            "[Epoch 17] loss: 1.198\n",
            "[Epoch 17] validation accuracy: 38.00 %\n",
            "[Epoch 18] loss: 1.446\n",
            "[Epoch 18] validation accuracy: 40.00 %\n",
            "[Epoch 19] loss: 1.307\n",
            "[Epoch 19] validation accuracy: 38.00 %\n",
            "Early stopping: validation accuracy did not improve for 10 epochs.\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daeaa59a-241c-453f-b00a-e17fe318b9c7",
        "id": "k7HMtGNBg9h5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 23 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb19639-ebf7-4ff2-9f9a-9ae354550ac6",
        "id": "zNyvwE-Hg9h5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of plane :  5 %\n",
            "Accuracy of   car : 46 %\n",
            "Accuracy of  bird : 20 %\n",
            "Accuracy of   cat : 35 %\n",
            "Accuracy of  deer : 37 %\n",
            "Accuracy of   dog : 21 %\n",
            "Accuracy of  frog :  9 %\n",
            "Accuracy of horse : 31 %\n",
            "Accuracy of  ship :  0 %\n",
            "Accuracy of truck : 28 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   ResNet50 (ImageNet weights and Image Augmentations)  | 9 | 42% | 23% |"
      ],
      "metadata": {
        "id": "7k8Di14eg9h6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRUA5I8N_2hk"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmyiWAPJ_2hl"
      },
      "source": [
        "__Question 7 (5 points) :__ Write a short report explaining the pros and the cons of each method that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ-v4Nev_2hl"
      },
      "source": [
        "In this project, we evaluated five different methods for image classification on the CIFAR-10 dataset. We implemented a Convolutional Neural Network (CNN), transfer learning using ResNet18 (with and without pre-trained weights), and ResNet50 with pre-trained weights (with and without image augmentations).\n",
        "\n",
        "The CNN achieved an accuracy of around 18% on the test set, which is relatively low. CNNs are suitable for image classification tasks because they can capture spatial relationships between the pixels, but they require a large amount of training data and are computationally expensive. Another disadvantage of CNNs is that they can overfit if not regularized properly.\n",
        "\n",
        "The transfer learning method using ResNet18 achieved the highest accuracy of around 29% on the test set when using pre-trained weights. Without pre-trained weights, the accuracy was around 22%. ResNet18 is a relatively shallow model compared to other popular architectures but is designed to handle the vanishing gradient problem that occurs in deep neural networks. This allows us to train deeper models without overfitting. Transfer learning with ResNet18 is more efficient than training a CNN from scratch and requires less training data. However, fine-tuning a pre-trained model requires more expertise, and the model may require significant computational resources.\n",
        "\n",
        "The transfer learning method using ResNet50 achieved the highest accuracy of around 30% on the test set when using pre-trained weights. However, with image augmentations, the accuracy dropped to around 23%. Increasing the size of the training dataset with more augmented images could potentially increase accuracy, but it may also overfit the data. ResNet50 is a deeper model than ResNet18, which makes it more powerful in capturing complex features. However, it is also more computationally expensive and requires more memory than ResNet18.\n",
        "\n",
        "Overall, the transfer learning methods outperformed the other methods in terms of accuracy. However, they require more computational resources and expertise to implement. The CNN is a good balance between simplicity and performance, but it requires a large amount of training data. Choosing a suitable method depends on the available resources, the desired performance, and the complexity of the task. For instance, if computational resources are limited, using a shallow model like ResNet18 would be a good choice. However, if high accuracy is desired, using ResNet50 with pre-trained weights could be a better option.\n",
        "\n",
        "Below is a plot summarizing the accuracy results of each method on the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   BaseLine CNN  | 12 | 20% | 18% |\n",
        "|   ResNet18 (Without Pre-Trained)  | 6 | 32% | 22% |\n",
        "|   ResNet18 (With Pre-Trained)  | 10 | 50% | 29% |\n",
        "|   ResNet50 (ImageNet weights)  | 14 | 38% | 30% |\n",
        "|   ResNet50 (ImageNet weights and Image Augmentations)  | 9 | 42% | 23% |"
      ],
      "metadata": {
        "id": "AAf2tk98BdwP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAGp7ddN_2hl"
      },
      "source": [
        "# Weak supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHQRLbC3_2hl"
      },
      "source": [
        "__Bonus \\[open\\] question (up to 3 points) :__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbjhfIvN_2hl"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "664bef82b06a4ebda0bd919b0e0efe93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6cd510c2ad64c8b86aa3903c928a93b",
              "IPY_MODEL_09b69c8ad0fb42c491cc08ac427e8be8",
              "IPY_MODEL_59f73bf5ad3446b1962830676993355e"
            ],
            "layout": "IPY_MODEL_880ff6e6bb714bb7a33dbce4dbc3a76c"
          }
        },
        "b6cd510c2ad64c8b86aa3903c928a93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4172f32c24b34f1f831ae36e16d54a1b",
            "placeholder": "​",
            "style": "IPY_MODEL_a4ada85d2ea54b44a8d7f8e7366e1473",
            "value": "100%"
          }
        },
        "09b69c8ad0fb42c491cc08ac427e8be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1320e9c258d438aab27dc593fe9bb85",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_413dec040714434b9e2065b5d377d256",
            "value": 170498071
          }
        },
        "59f73bf5ad3446b1962830676993355e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4658b4c556474f89cf54f122a83aff",
            "placeholder": "​",
            "style": "IPY_MODEL_dd9be09960f64fe4b9ee3baec74379cb",
            "value": " 170498071/170498071 [00:14&lt;00:00, 14041328.97it/s]"
          }
        },
        "880ff6e6bb714bb7a33dbce4dbc3a76c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4172f32c24b34f1f831ae36e16d54a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ada85d2ea54b44a8d7f8e7366e1473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1320e9c258d438aab27dc593fe9bb85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413dec040714434b9e2065b5d377d256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e4658b4c556474f89cf54f122a83aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9be09960f64fe4b9ee3baec74379cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc66c381222945e096fa1546581d4e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7932e4bb937a4d539081b11a934b0892",
              "IPY_MODEL_32ca89a57cc14e8a9589147e76fa9bc5",
              "IPY_MODEL_dcc7f3bbde8742f69c519a044c3c551c"
            ],
            "layout": "IPY_MODEL_4be84794e76947a1938fed3fc8cca790"
          }
        },
        "7932e4bb937a4d539081b11a934b0892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2efd162568be485f9e14c5f81a9323af",
            "placeholder": "​",
            "style": "IPY_MODEL_17a8f47e81bf4210af52029641173a7e",
            "value": "100%"
          }
        },
        "32ca89a57cc14e8a9589147e76fa9bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e510d8729fa34b44853cf10a02a66cec",
            "max": 102540417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3e8ab9e461c4a59a33572df5edc6568",
            "value": 102540417
          }
        },
        "dcc7f3bbde8742f69c519a044c3c551c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0935ceb23e494f2f9107db6dabf75a1a",
            "placeholder": "​",
            "style": "IPY_MODEL_217533695ef8487aa5e8a5aed4eac07e",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 295MB/s]"
          }
        },
        "4be84794e76947a1938fed3fc8cca790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efd162568be485f9e14c5f81a9323af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a8f47e81bf4210af52029641173a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e510d8729fa34b44853cf10a02a66cec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e8ab9e461c4a59a33572df5edc6568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0935ceb23e494f2f9107db6dabf75a1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217533695ef8487aa5e8a5aed4eac07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}